---
title: "Optimization"
author: "Stefan Glogger"
date: "August 2017"
output: pdf_document
---

```{r, include=FALSE}
source("parameters.R")
```

```{r, include=FALSE, cache=T, results='hide'}
knitr::knit_child("30 Data Visualization.Rmd")
```


# Optimization of Portfolios


## classic portfolio optimization

First of all, we do a classic portfolio optimization. We start of with a mean variance diagram.

### notation

Let $x = (x_1, ..., x_p)^T$ represent the portfolio ($x_i$ is percentage of available capital invested in security $i$). Therefore it holds $\sum_{i=1}^p x_i = 1$. Note, that short selling is allowed.

Let $R = (R_1, ..., R_p)^T$ represent the annual returns ($R_i$ is return of security $i$). And let $\mu = (\mu_1, ..., \mu_p)^T$ represent the expected returns ($\mu_i = \text{E}[R_i] > 0$).

Furthermore $C = (c_{ij})_{i,j \in \{1,...,p\}}$ denotes the (annual) covariance matrix ($c_{ij} = \text{Cov}(R_i, R_j)$).

Then we have Return $R(x)$ of portfolio $x$ given by $R(x) = \sum_{i=1}^p x_i R_i = x^T R$.

The expected return $\mu(x)$ of portfolio $x$ is given by $\mu(x) = \text{E}[R(x)] = \sum_{i=1}^p x_i \mu_i = x^T \mu$.

The Variance $\sigma^2(x)$ of portfolio $x$ is given by $\sigma^2(x) = \text{Var}(R(x)) = \text{E}[(R(x) - \text{E}(R(x)))^2] = x^T C x$.

We therefore annualize the returns and the variance.

```{r}
anRet <- (1+ret)^52-1
anMu <- (1+mu)^52-1
anC <- C*52
```


### mean variance diagram

We plot *K* random portfolios.

#### with riskless asset

```{r}
set.seed(1)
K <- 10000

mvRandom <- matrix(0, ncol = 2, nrow = K)
for(i in 1:nrow(mvRandom)){
    x <- rnorm(ncol(ret))
    x <- x/sum(x) # normalize
    
    mvRandom[i, 1] <- sum(x*anMu)
    mvRandom[i, 2] <- sqrt((x%*%anC)%*%x)
}

plot(mvRandom[,2], mvRandom[,1], 
     xlab = "standard deviation", ylab = "expected return",
     xlim = c(0, 0.25), ylim = c(0, 0.5))
```


#### exclude riskless assets

exclude riskless asset (BUND)

```{r}
retRisky <- ret[,-7]
colnames(retRisky)

muRisky <- colMeans(retRisky)
CRisky <- cov(retRisky)

anRetRisky <- (1+retRisky)^52-1
anMuRisky <- (1+muRisky)^52-1
anCRisky <- CRisky*52
```


```{r}
set.seed(1)
K <- 10000

mvRandom <- matrix(0, ncol = 2, nrow = K)
for(i in 1:nrow(mvRandom)){
    x <- rnorm(ncol(retRisky))
    x <- x/sum(x) # normalize
    
    mvRandom[i, 1] <- sum(x*anMuRisky)
    mvRandom[i, 2] <- sqrt((x%*%anCRisky)%*%x)
}

plot(mvRandom[,2], mvRandom[,1], 
     xlab = "standard deviation", ylab = "expected return",
     xlim = c(0, 0.25), ylim = c(0, 0.5))
```

### efficiency without risk free portfolio

We can use theorem 2.2. of Portfolio Analysis (slide 40). But be careful as C is close to singular.

efficiency line by formula d)


```{r}
det(anC)
det(anCRisky)
```

```{r}
anCRisky1 <- solve(anCRisky)
anCRisky %*% anCRisky1

a <- sum(anCRisky1 %*% anMuRisky)
b <- c((anMuRisky %*% anCRisky1) %*% anMuRisky)
c <- sum(anCRisky1)
d <- b*c - a^2
```

```{r}
set.seed(1)
K <- 10000

mvRandom <- matrix(0, ncol = 2, nrow = K)
for(i in 1:nrow(mvRandom)){
    x <- rnorm(ncol(retRisky))
    x <- x/sum(x) # normalize
    
    mvRandom[i, 1] <- sum(x*anMuRisky)
    mvRandom[i, 2] <- sqrt((x%*%anCRisky)%*%x)
}

plot(mvRandom[,2], mvRandom[,1], 
     xlab = "standard deviation", ylab = "expected return",
     xlim = c(0, 0.25), ylim = c(0, 0.5))

k <- 100
elWithout <- matrix(0, ncol = 2, nrow = k)
elWithout[,2] <- seq(sqrt(1/c), 0.5, length.out = k)
for(i in 1:nrow(elWithout)){
    elWithout[i,1] <- a/c + sqrt(d/c*(elWithout[i,2]^2 - 1/c))
}
par(new=T)
plot(elWithout[,2], elWithout[,1], type = "l", col = "blue",
     axes = FALSE, xlab = "", ylab = "",
     xlim = c(0, 0.25), ylim = c(0, 0.5))

par(new=T)
plot(sqrt(1/c), a/c,
     col = "blue", pch = 4, lwd = 2,
     axes = FALSE, xlab = "", ylab = "",
     xlim = c(0, 0.25), ylim = c(0, 0.5))
```

```{r}
(xMVPwithoutRF <- 1/c*rowSums(anCRisky1))
c(a/c, xMVPwithoutRF %*% anMuRisky)
c(sqrt(1/c), sqrt( (xMVPwithoutRF%*%anCRisky)) %*% xMVPwithoutRF)
```

### efficiency with risk free portfolio

assume BOND to be risk free

```{r}
r <- anMu[7]

set.seed(1)
K <- 10000

mvRandom <- matrix(0, ncol = 2, nrow = K)
for(i in 1:nrow(mvRandom)){
    x <- rnorm(ncol(retRisky))
    x <- x/sum(x) # normalize
    
    mvRandom[i, 1] <- sum(x*anMuRisky)
    mvRandom[i, 2] <- sqrt((x%*%anCRisky)%*%x)
}

plot(mvRandom[,2], mvRandom[,1], 
     xlab = "standard deviation", ylab = "expected return",
     xlim = c(0, 0.25), ylim = c(0, 0.5))

k <- 100
elWithout <- matrix(0, ncol = 2, nrow = k)
elWithout[,2] <- seq(sqrt(1/c), 0.5, length.out = k)
for(i in 1:nrow(elWithout)){
    elWithout[i,1] <- a/c + sqrt(d/c*(elWithout[i,2]^2 - 1/c))
}
par(new=T)
plot(elWithout[,2], elWithout[,1], type = "l", col = "blue",
     axes = FALSE, xlab = "", ylab = "",
     xlim = c(0, 0.25), ylim = c(0, 0.5))

par(new=T)
plot(sqrt(1/c), a/c,
     col = "blue", pch = 4, lwd = 2,
     axes = FALSE, xlab = "", ylab = "",
     xlim = c(0, 0.25), ylim = c(0, 0.5))

elWith <- matrix(0, ncol = 2, nrow = k)
elWith[,2] <- seq(0, 0.5, length.out = k)
for(i in 1:nrow(elWith)){
    elWith[i,1] <- r + elWith[i,2]*sqrt(c*r^2 - 2*a*r + b)
}
par(new=T)
plot(elWith[,2], elWith[,1], type = "l", col = "green",
     axes = FALSE, xlab = "", ylab = "",
     xlim = c(0, 0.25), ylim = c(0, 0.5))
```

```{r}
(xMarket <- 1/(a-c*r)*anCRisky1%*%(anMuRisky-r))
unname((b-a*r)/(a-c*r))
unname((c*r^2 - 2*a*r + b)/(a-c*r)^2)
```

### cleanup

```{r}
rm(a, anCRisky1, b, c, d, elWith, elWithout, i, k, K, mvRandom, r, x)
rm(anC, anCRisky, anRet, anRetRisky, CRisky, xMarket, anMuRisky, muRisky, retRisky, xMVPwithoutRF)
rm(compareDispRes)
```




\newpage
## with sentiment (grid search)

IDEE: one could also look at just the previous *n* dates to calculate the average annual quantities.

### general setup

We use several packages for the optimization.

```{r}
library(Rdonlp2)
```


Setup Grid. Take care that weights sum up to 1, each weight is at least *wmin* and at most *wmax*.

```{r}
stepsPerWeight <- 19
wmin <- 0.05
wmax <- 0.95
weights <- seq(wmin, wmax, length.out = stepsPerWeight)
grid <- expand.grid(w1 = weights, w2 = weights, w3 = weights )
grid <- grid[abs(rowSums(grid) - 1.0) < 0.0001,]
rownames(grid) <- 1:nrow(grid)

nrow(grid)
rm(stepsPerWeight, wmin, wmax, weights)
```

With this setup, we have `r nrow(grid)` combinations of weights.


Overview of what data we use.

```{r}
targetRpa
targetVolpa
targetDisp

IneqA <- matrix(1, nrow = 1, ncol = ncol(ret)) # to take care of investments
```



### dispersion direct min

We handle dispersion like return in the first place. Therefore we have the following objective functions:

\begin{tabular}{lll}
1. & return & max$\left(w_1 \cdot \frac{x^T\mu}{\mu_{target}}\right)$\\
2. & volatility & min$\left(w_2 \cdot \frac{\sqrt{x^T C x}}{\sigma_\text{target}}\right)$)\\
3. & dispersion & min$\left(w_3 \cdot \frac{x^T\text{d}}{\text{d}_\text{target}}\right)$
\end{tabular}

where $\text{d}$ denotes the annualized dispersion of each index, we name it *anDisp*. We furthermore assume that the annual dispersion equals the average dispersion.

```{r}
anDisp <- lapply(sDisp, function(x) {colMeans(x[,-1])})
```

We will minimize the following objective function. Be aware that maximizing something equals minimizing its negative. Furthermore *anDOpt* denotes the annualized dispersion of the indizes. We divide by the target values to have the different components of the objective function comparable (in units of the corresponding target value). We denote *Opt* to be the (newly calculated) data.

```{r}
hDispersionDirectMin <- function(x){
    y <- numeric(3)
    y[1] <- -1.0 * w[1] * drop(crossprod(x, anMuOpt)) / targetRpa
    y[2] <- w[2] * drop(sqrt(t(x) %*% anCOpt %*% x)) * sqrt(12) / targetVolpa
    y[3] <- w[3] * drop(crossprod(x, anDOpt)) / targetDisp
    return(sum(y))
}
```



#### constant portfolio weights over time window

##### determine weights

NOTE: We keep structure, as we might change lateron the test window to come up with different weights for the different time periods (one test window to get weights for bear market, another test window to get weights for bull market, ...)
NOTE: Then also adopt for dopar



First, we fix the weights $x_i$ of each security at the beginning of (at the date before) the time window and keep them constant over time.

We store our results in the following data structure (levels of list), while having in mind that we might create a ternary plot lateron (therefore weights inside).

time window -> dispersion (sentixDataNames) -> weights of goal function -> weights of assets

We store the solution (the weights of assets), the objective value and the time needed for the computation (in seconds).

Work in parallel.

```{r}
library(foreach)
library(parallel) # detectCores()
library(doSNOW)
```

We save with saveRDS() to be able to import and compare different results.

```{r, cache=TRUE}
cores <- detectCores()

if(Sys.getenv("USERNAME") == "Stefan"){
    cl <- makeCluster(cores - 1)
} else if(Sys.getenv("USERNAME") == "gloggest"){
    cl <- makeCluster(cores) # use server fully
} else
    stop("Who are you???")


xDispConstTest <- list()

registerDoSNOW(cl)
xDispConstTest <- foreach(t = datesTestNames, .export = c(datesTestNames), .packages = c("Rdonlp2")) %dopar%{
    L <- list()
    timeInd <- which(datesAll == min(get(t)))-1 ## one day before start of time window
    
    retOpt <- ret[1:timeInd,]
    anMuOpt <- (1+colMeans(retOpt))^52-1
    anCOpt <- cov(retOpt)*52
    
    for(i in names(sDisp)){
        anDOpt <- colMeans(sDisp[[i]][1:timeInd,-1])
        
        for(weightInd in 1:nrow(grid)){
            w <- unlist(grid[weightInd,])
            
            erg <- donlp2NLP(start = rep(1/ncol(retOpt), ncol(retOpt)), fun = hDispersionDirectMin, 
                      par.lower = rep(0, ncol(retOpt)), ineqA = IneqA, 
                      ineqA.lower = 1.0, ineqA.upper = 1.0)
            L[[i]][[paste(w, collapse = "-")]] <- list(x = erg$solution, obj = erg$objective, time = as.numeric(erg$elapsed))
        }
    }
    L
}
stopCluster(cl)

names(xDispConstTest) <- datesTestNames

saveRDS(xDispConstTest, file = file.path(getwd(), "Optimization", paste0("EDispersionMinConstantTest_", Sys.getenv("USERNAME"), format(Sys.time(), "%Y-%m-%d---%H-%M"))))
```


We now add the returns and the variance over the test time window to lateron calculate the Sharpe Ratio and determine the best weights for each sentiment group.

The function takes the calculated weighted of the assets as inputs and outputs (in the same data structure) the portfolio weights, its return and its variance over the test time window.



```{r}
calcEvalTestConst <- function(dat){
    res <- list()
    for(t in names(dat)){
        retTest <- ret[get(t),]
        muTest <- apply((1+retTest), 2, function(x) {prod(x)-1}) # total return (over whole period)
        sigmaTest <- cov(ret) # variance (over whole period)
        rf <- muTest["BUND"]
        
        res[[t]] <- lapply(dat[[t]], function(x) {
            lapply(x, function(y){
                list(r <- (crossprod(y$x, muTest)-rf), sd <- sqrt(y$x %*% sigmaTest %*% y$x), 
                     sr = r/sd, fweight = y$obj)
            })
        })
    }
    return(res)
}
temp <- calcEvalTestConst(xDispConstTest)
```

PROBLEM: from *t=1* to *t=50*, we have negative returns of the stocks, therefore, we invest fully in BUND, if we put enough weight on return. This is not, what we want => go directly to different portfolio weights over (test) time window


```{r}
rm(temp, cl)
```


#### different portfolio weights over time window

We evaluate an optimal portfolio at each date within our time period and assume that we can redistribute our wealth at no cost.

We use a moving time window of *k* dates before the actual date to determine mean and variance and therefore to determine the portfolio. Furthermore, we just use the actual dispersion.

We move the parallelization further inside to be sure that we make use of parallelization (might just have one test window).

End result has the following structure:
time window -> dispersion (sentixDataNames) -> weights of goal function -> dates in time window -> weights of assets

Last weight for penultimate date of time window (hold until last date).

##### Portfolio Weights

```{r}
k <- 50

cores <- detectCores()

if(Sys.getenv("USERNAME") == "Stefan"){
    cl <- makeCluster(cores - 1)
} else if(Sys.getenv("USERNAME") == "gloggest"){
    cl <- makeCluster(cores) # use server fully
} else
    stop("Who are you???")


xDispVarTest <- list()

for(t in datesTestNames){
    
    xDispVarTest[[t]] <- foreach(i = names(sDisp), .export = c(datesTestNames), .packages = c("Rdonlp2")) %dopar%{
        L <- list()
        
        for(weightInd in 1:nrow(grid)){
            w <- unlist(grid[weightInd,])
            
            mat <- matrix(NA, nrow = (length(get(t))-1), ncol = ncol(ret))
            colnames(mat) <- colnames(ret)
            rownames(mat) <- get(t)[1:(length(get(t))-1)]
            obj <- numeric(length(get(t))-1)
            tim <- numeric(length(get(t))-1)
            
            # first separate to then use the previous solution as starting point for next solution
            ### -------------------
            j <- 1
            tInd <- which(datesAll == get(t)[j])
            retOpt <- ret[(tInd-k+1):tInd,]
            anMuOpt <- (1+colMeans(retOpt))^52-1
            anCOpt <- cov(retOpt)*52
            anDOpt <-  as.numeric(sDisp[[i]][tInd,-1])
            
            erg <- donlp2NLP(start = rep(1/ncol(retOpt), ncol(retOpt)), fun = hDispersionDirectMin, 
                             par.lower = rep(0, ncol(retOpt)), ineqA = IneqA, 
                             ineqA.lower = 1.0, ineqA.upper = 1.0)
            mat[1,] <- erg$solution
            obj[1] <- erg$objective
            tim[1] <- as.numeric(erg$elapsed)
            ### -------------------
            
            for(j in 2:(length(get(t))-1)){
                tInd <- which(datesAll == get(t)[j])
                retOpt <- ret[(tInd-k+1):tInd,]
                anMuOpt <- (1+colMeans(retOpt))^52-1
                anCOpt <- cov(retOpt)*52
                anDOpt <- as.numeric(sDisp[[i]][tInd,-1])
                
                erg <- donlp2NLP(start = mat[j-1,], fun = hDispersionDirectMin, 
                                 par.lower = rep(0, ncol(retOpt)), ineqA = IneqA, 
                                 ineqA.lower = 1.0, ineqA.upper = 1.0)
                mat[j,] <- erg$solution
                obj[j] <- erg$objective
                tim[j] <- as.numeric(erg$elapsed)
            }
            
            L[[paste(w, collapse = "-")]] <- list(x = mat, obj = obj, time = tim)
            print(weightInd/nrow(grid))
        }
        L
    }
    names(xDispVarTest[[t]]) <- names(sDisp)
}

saveRDS(xDispVarTest, file = file.path(getwd(), "Optimization", paste0("EDispersionMinVaryingTest_", Sys.getenv("USERNAME"), format(Sys.time(), "%Y-%m-%d---%H-%M"))))

stopCluster(cl)
```

```{r}
xDispVarTest <- readRDS(file.path(getwd(), "Optimization", "EDispersionMinVaryingTest_gloggest2017-08-28---11-03"))
```


```{r}
rm(erg)
```




##### Goal weights

We have the portfolio weights for each date (start) in "dat\$datesTest\$P1\$\`0.9-0.05-0.05\`\$x". We hold this for one period, therefore we calculate the portfolio return at each time step.

The datastructure is the following

dat -> datesTest -> P1 -> 0.9-0.05-0.05 -> x

We want the datastrucute

dat -> datesTest -> P1 -> 0.9-0.05-0.05 -> r, sd, sr, fweight

with

r: return (overall)
sd: standard deviation (overall)
sr: sharpe ratio
fweight: mean of goal function

For the calculation of r, we procede as: $\mu = \text{E}[\text{R}] = \sum_{t=1}^T \text{R}_t$
and for sd: $\text{sd}(\text{R})=\sqrt{\text{Var}(\text{R})}$

In *ret* we have how much return was done at the current date (row), so we have to make a one period shift (hold portfolio up to next period and the return we make is given as the next stored return).

```{r}
calcTestVar <- function(dat){
    res <- list()
    for(timeWindowName in names(dat)){
        timeWindow <- get(timeWindowName)
        retTimeWindow <- ret[timeWindow,]
        retTimeWindow <- retTimeWindow[-1,]
        colnames(retTimeWindow) <- colnames(ret)
        
        rf <- mean(retTimeWindow[,"BUND"])
        
        for(sentixGroupName in names(dat[[timeWindowName]])){
            
            for(goalWeight in names(dat[[timeWindowName]][[sentixGroupName]])){
                R <- rowSums(dat[[timeWindowName]][[sentixGroupName]][[goalWeight]]$x * retTimeWindow)
                
                r <- mean(R)
                sd <- sd(R)
                
                anR <- (1+r)^52-1
                anSd <- sqrt((sd^2)*52)
                
                fweight = mean(dat[[timeWindowName]][[sentixGroupName]][[goalWeight]]$obj)
                    
                res[[timeWindowName]][[sentixGroupName]][[goalWeight]] <- list(r = r, sd = sd, sr = r/sd,
                                                                               anR = anR, anSd = anSd, anSR = anR/anSd,
                                                                               fweight = fweight)
            }
        }
    }
    return(res)
}
```

```{r}
xDispVarTestCalc <- calcTestVar(xDispVarTest)
```


now, determine optimal weight, for each *sentixGroup* and *timeWindow*, optimal meaning maximum sharpe ratio


Now, we also want to visualize the data. We use a ternary plot for this. 

```{r}
library(ggtern)
```


```{r}
extractWeightsWithValue <- function(dat, value){
    ret <- list()
    for(timeWindowName in names(dat)){
        for(sentixGroupName in names(dat[[timeWindowName]])){
            df <- data.frame(w = names(dat[[timeWindowName]][[sentixGroupName]])[1], value = dat[[timeWindowName]][[sentixGroupName]][[1]][[value]])
            df$w <- as.character(df$w)
            df$w1 <- as.numeric(unlist(strsplit(df$w, "-"))[1])
            df$w2 <- as.numeric(unlist(strsplit(df$w, "-"))[2])
            df$w3 <- as.numeric(unlist(strsplit(df$w, "-"))[3])
            
            for(weightsName in names(dat[[timeWindowName]][[sentixGroupName]])[2:length(names(dat[[timeWindowName]][[sentixGroupName]]))]){
                df <- rbind(df, c(weightsName, dat[[timeWindowName]][[sentixGroupName]][[weightsName]][[value]], unlist(strsplit(weightsName, "-"))))
            }
            
            ret[[timeWindowName]][[sentixGroupName]] <- data.frame(w1 = as.numeric(df[,"w1"]), 
                                                                         w2 = as.numeric(df[,"w2"]), 
                                                                         w3 = as.numeric(df[,"w3"]),
                                                                         value = as.numeric(df[,"value"]))
        }
    }
    return(ret)
}

srWeightsAn <- extractWeightsWithValue(xDispVarTestCalc, "anSR")
```

```{r}
terntheme <- function(){
  list(theme_rgbg(), theme(legend.position = c(0, 1),legend.justification = c(0, 1),
                           plot.margin=unit(c(0, 2,0, 2), "cm")))
}
```

note: "Calling 'structure(NULL, \*)' is deprecated, as NULL cannot have attributes. Consider 'structure(list(), \*)' instead." is just a message, not an error. it is due to strict checks in the newer R-version <https://stat.ethz.ch/pipermail/r-devel/2016-December/073554.html>

*..level..* is calculated and then used inside the function

```{r}


```


```{r}
plotTernary <- function(dat){
    wmax <- dat[which.max(dat$value), c("w1", "w2", "w3")]
    
    ggtern(dat, aes(x=w1, y=w2, z=w3, value=value)) +
        geom_point(shape=".")+
        geom_text(aes(x=w1, y=w2, label=round(value,1)), data = dat[dat$value>(dat[(dat$w1==wmax$w1) & (dat$w2==wmax$w2),"value"]*0.95),]) +
        geom_interpolate_tern(aes(value=value, color = ..level..)) +
        geom_point(aes(x=w1, y=w2), dat = wmax, color = "red") + 
        geom_text(aes(x=w1, y=w2, label=round(dat[(dat$w1==wmax$w1) & (dat$w2==wmax$w2),"value"],1)), data = wmax, color = "red" )+
        terntheme() +
        Lline(Lintercept =  wmax$w1, colour = theme_rgbg()$tern.axis.line.L$colour, linetype = 2, lwd=1) + ## w1 on Left axis
        Tline(Tintercept = wmax$w2, colour = theme_rgbg()$tern.axis.line.T$colour, linetype = 2, lwd=1) + ## w2 on Top axis
        Rline(Rintercept = wmax$w3, color = theme_rgbg()$tern.axis.line.R$colour, linetype = 2, lwd=1) + ## w3 on Right axis
        scale_color_gradient(low = "green", high = "red") +
        labs(x = "return", y = "variation", z = "dispersion",
             title = paste0("Ternary Plot with Sharpe Ratio Contour Lines -", deparse(substitute(dat)), "-", t),
             color = "Level")
}
```

```{r}
for(t in names(srWeightsAn$datesTest)){
    plot(plotTernary(srWeightsAn$datesTest[[t]]))
    
    pdf(file.path(getwd(), "Plot", paste0("Ternary - Weights ",t, ".pdf")), width = 10, height = 10)
    plot(plotTernary(srWeightsAn$datesTest[[t]]))
    dev.off()
}
```




now, determine optimal weight, for each *sentixGroup* and *timeWindow*, optimal meaning maximum sharpe ratio


```{r}
wOptSRAn <- list()
for(timeWindowName in names(srWeightsAn)){
    for(sentixGroupName in names(srWeightsAn[[timeWindowName]])){
        df <- srWeightsAn[[timeWindowName]][[sentixGroupName]]
        
        wOptSRAn[[timeWindowName]][[sentixGroupName]] <- df[which.max(df$value), c("w1", "w2", "w3")]
    }
}
wOptSRAn$datesTest
```


##### determine portfolios for time windows

Now, we want to determine the portfolio weights over the time Windows. We use the data structure:

time window -> dispersion (sentixGroup) -> weights of assets

NOTE: change the weights of the test data, if different test time windows are used to get weights for different eval time weights

We use a moving time window of *k* dates before the actual date to determine mean and variance and therefore to determine the portfolio. Furthermore, we just use the actual dispersion.


```{r}
k <- 50

cores <- detectCores()

if(Sys.getenv("USERNAME") == "Stefan"){
    cl <- makeCluster(cores - 1)
} else if(Sys.getenv("USERNAME") == "gloggest"){
    cl <- makeCluster(cores) # use server fully
} else
    stop("Who are you???")


xSentixVarying <- list()

registerDoSNOW(cl)
for(t in datesEvalNames){
    # timeInd <- datesAll[which(datesAll == min(get(t)))-1] ## one day before start of time window ### not used any more, as we move up to actual date
    
    xSentixVarying[[t]] <- foreach(i = names(sDisp), .export = c(datesEvalNames), .packages = c("Rdonlp2")) %dopar%{
        L <- list()
        
        
        w <- unlist(wOptSRAn$datesTest[[i]])
        
        mat <- matrix(NA, nrow = (length(get(t))-1), ncol = ncol(ret))
        colnames(mat) <- colnames(ret)
        rownames(mat) <- get(t)[1:(length(get(t))-1)]
        obj <- numeric(length(get(t))-1)
        tim <- numeric(length(get(t))-1)
        
        # first separate to then use the previous solution as starting point for next solution
        ### -------------------
        j <- 1
        tInd <- which(datesAll == get(t)[j])
        retOpt <- ret[(tInd-k+1):tInd,]
        anMuOpt <- (1+colMeans(retOpt))^52-1
        anCOpt <- cov(retOpt)*52
        anDOpt <-  as.numeric(sDisp[[i]][tInd,-1])
        
        erg <- donlp2NLP(start = rep(1/ncol(retOpt), ncol(retOpt)), fun = hDispersionDirectMin, 
                         par.lower = rep(0, ncol(retOpt)), ineqA = IneqA, 
                         ineqA.lower = 1.0, ineqA.upper = 1.0)
        mat[1,] <- erg$solution
        obj[1] <- erg$objective
        tim[1] <- as.numeric(erg$elapsed)
        ### -------------------
        
        for(j in 2:(length(get(t))-1)){
            tInd <- which(datesAll == get(t)[j])
            retOpt <- ret[(tInd-k+1):tInd,]
            anMuOpt <- (1+colMeans(retOpt))^52-1
            anCOpt <- cov(retOpt)*52
            anDOpt <- as.numeric(sDisp[[i]][tInd,-1])
            
            erg <- donlp2NLP(start = mat[j-1,], fun = hDispersionDirectMin, 
                             par.lower = rep(0, ncol(retOpt)), ineqA = IneqA, 
                             ineqA.lower = 1.0, ineqA.upper = 1.0)
            mat[j,] <- erg$solution
            obj[j] <- erg$objective
            tim[j] <- as.numeric(erg$elapsed)
        }
        
        list(x = mat, obj = obj, time = tim)
    }
    names(xSentixVarying[[t]]) <- names(sDisp)
}

stopCluster(cl)

# names(xDispConstTest) <- datesTestNames

saveRDS(xSentixVarying, file = file.path(getwd(), "Optimization", paste0("EDispersionMinVaryingEval_", Sys.getenv("USERNAME"), format(Sys.time(), "%Y-%m-%d---%H-%M"))))

```



## without sentiment (classic)

### constant portfolio

We also do some classical portfolio optimization, namely 

\begin{tabular}{llll}
1. & tangency portfolio & fPortfolio & highest return/risk ratio on the efficient frontier (market portfolio)\\
2. & minimum variance & fPortfolio & portfolio with minimal risk on the efficient frontier\\
3. & rp & cccp & risk parity solution of long-only portfolio\\
4. & PGMV & FRAPO (Pfaff) & global minimum variance (via correlation)\\
5. & PMD & FRAPO (Pfaff) & most diversivied portfolio (long-only)\\
6. & ew & own & equal weight
\end{tabular}

safe results in *xClassicConst* in an anolous manner to above 

time window -> portfolio optimizing -> weights of assets

Be aware that the portfolios work with time series and therefore some typecasting is necessary.

```{r}
library(fPortfolio)
library(FRAPO)
```

```{r}
xClassicConst <- list()

# convert rownames back to date format (character!)
t <- rownames(ret) 
class(t) <- "Date"
rdatTimeSource <- timeSeries(ret, charvec = as.character(t))

# equal weights to start with (maybe)
ew <- rep(1/ncol(ret), ncol(ret))

for(t in datesEvalNames){
    timeInd <- datesAll[which(datesAll == min(get(t)))-1] ## one day before start of time window
    
    rdatTime <- window(rdatTimeSource, start = start(rdatTimeSource), end = timeInd) # note: first day is missing as returns miss first day of datesAll (naturally as no return at start)
    
    ans <- tangencyPortfolio(rdatTime)
    xClassicConst[[t]][["tanPort"]] <- getWeights(ans)
    
    ans <- minvariancePortfolio(rdatTime)
    xClassicConst[[t]][["mVaPort"]] <- getWeights(ans)
    
    C <- cov(rdatTime)
    ans <- rp(ew, C, ew, optctrl = ctrl(trace = FALSE)) 
    xClassicConst[[t]][["rp"]] <- c(getx(ans))
    
    ans <- PGMV(rdatTime, optctrl = ctrl(trace = FALSE))
    xClassicConst[[t]][["PGMV"]] <- Weights(ans) / 100
    
    ans <- PMD(rdatTime, optctrl = ctrl(trace = FALSE))
    xClassicConst[[t]][["PMD"]] <- Weights(ans) / 100
    
    xClassicConst[[t]][["ew"]] <- ew
}
```





### different portfolio weights over time window

IDEA: look at portfolio-rollingPortfolios {fPortfolio}


manually rolling

safe results in *xClassicVar* in an anolous manner to above 

time window -> portfolio (classic) -> weights of assets

NOTE: change the weights of the test data, if different test time windows are used to get weights for different eval time weights

We use a moving time window of *k* dates before the actual date to determine mean and variance and therefore to determine the portfolio. Furthermore, we just use the actual dispersion.




```{r}
k <- 50

xClassicVar <- list()

# convert rownames back to date format (character!)
t <- rownames(ret) 
class(t) <- "Date"
rdatTimeSource <- timeSeries(ret, charvec = as.character(t))

# equal weights to start with (maybe)
ew <- rep(1/ncol(ret), ncol(ret))

for(timeWindowName in datesEvalNames){
    datesEvalNow <- get(timeWindowName)
    
    mat <- matrix(NA, nrow = (length(datesEvalNow)-1), ncol = ncol(rdatTimeSource))
    colnames(mat) <- colnames(ret)
    rownames(mat) <- datesEvalNow[1:(length(datesEvalNow)-1)]
    
    xClassicVar[[timeWindowName]][["tanPort"]]$x <- mat
    xClassicVar[[timeWindowName]][["mVaPort"]]$x <- mat 
    xClassicVar[[timeWindowName]][["rp"]]$x <- mat 
    xClassicVar[[timeWindowName]][["PGMV"]]$x <- mat 
    xClassicVar[[timeWindowName]][["PMD"]]$x <- mat 
    xClassicVar[[timeWindowName]][["ew"]]$x <- mat 
    
    for(d in 1:(length(datesEvalNow)-1)){ # last date no portfolio weights
        
        timeEndInd <- which(datesAll == datesEvalNow[d]) ## one day before start of time window => NO, we can go up to the date
        timeEnd <- datesAll[timeEndInd]
        timeStart <- datesAll[timeEndInd-k+1]

        rdatTime <- timeSeries::window(rdatTimeSource, start = timeStart, end = timeEnd) # note: first day is missing as returns miss first day of datesAll (naturally as no return at start)
        
        ans <- tangencyPortfolio(rdatTime)
        xClassicVar[[timeWindowName]][["tanPort"]]$x[d,] <- getWeights(ans)
        
        ans <- minvariancePortfolio(rdatTime)
        xClassicVar[[timeWindowName]][["mVaPort"]]$x[d,] <- getWeights(ans)
        
        C <- cov(rdatTime)
        ans <- rp(ew, C, ew, optctrl = ctrl(trace = FALSE)) 
        xClassicVar[[timeWindowName]][["rp"]]$x[d,] <- c(getx(ans))
        
        ans <- PGMV(rdatTime, optctrl = ctrl(trace = FALSE))
        xClassicVar[[timeWindowName]][["PGMV"]]$x[d,] <- Weights(ans) / 100
        
        ans <- PMD(rdatTime, optctrl = ctrl(trace = FALSE))
        xClassicVar[[timeWindowName]][["PMD"]]$x[d,] <- Weights(ans) / 100
        
        xClassicVar[[timeWindowName]][["ew"]]$x[d,] <- ew
    }
}
```



```{r, results='hide', eval=FALSE}

Wmsr <- matrix(NA, nrow = length(datesEvalLast), ncol = numAsset)
Wmdp <- Wgmv <- Werc <- Wmsr

for(d in datesEvalLast){
    dInd <- which(datesEvalLast==d)
    class(d) <- "Date"
    rdatTime <- window(rdatTimeSource, start = start(rdatTimeSource), end = d-1) # just look at period before
    
    ans <- tangencyPortfolio(rdatTime)
    Wmsr[dInd, ] <- getWeights(ans)
    
     
    ### global minimum variance
    ans <- PGMV(rdatTime)
    Wgmv[dInd, ] <- FRAPO::Weights(ans) / 100
    
    ### most diversified
    ans <- PMD(rdatTime)
    Wmdp[dInd, ] <- FRAPO::Weights(ans) / 100
    
    ### risk parity optimization
    SStock <- cov(rdatTime)
    ans <- rp(ew, SStock, ew, optctrl = ctrl(trace = FALSE)) # maybe invisible() makes output silent
    Werc[dInd, ] <- c(getx(ans))
}

Eclassic <- list("MSR" = Wmsr, "MDP" = Wmdp, "GMV" = Wgmv, "ERC" = Werc)
```




# ----- TODO -----

```{r, eval=FALSE}
ergSentixNames <- c()
i = 1
parse(text = paste0("ergSentixNames <- ", "c(ergSentixNames, \"erg", sentixDataNames[i], "\")"))
for(i in sentixDataNames){
    eval(parse(text = paste0("ergSentixNames <- ", "c(ergSentixNames, \"erg", i, "\")")))
}
```




### mrc


start optimization with equal weights and then start each iteration with result of previous iteration


roughly 30 seconds per strategy and weight (on laptop stefan)
```{r, eval=FALSE}
nrow(grid)*length(sentixDataNamesReg)*30 # Sekunden
nrow(grid)*length(sentixDataNamesReg)*30/60 # Minuten
nrow(grid)*length(sentixDataNamesReg)*30/60/60 # Stunden
```

roughly 14 seconds per strategy and weight (on laptop stefan)
```{r, eval=FALSE}
nrow(grid)*length(sentixDataNamesReg)*14 # Sekunden
nrow(grid)*length(sentixDataNamesReg)*14/60 # Minuten
nrow(grid)*length(sentixDataNamesReg)*14/60/60 # Stunden
```


Generate a list holding all data with structure (levels of list)
weights of goal function -> strategy -> dates -> weights of assets

```{r, eval=FALSE}
# sentLookback <- 20
# 
# E <- list()
# tt <- numeric(nrow(grid)*length(sentixDataNamesReg)) # track time to evaluate code
# 
# for(weightInd in 1:nrow(grid)){
#     w <- as.numeric(grid[weightInd,])
#     weightName <- paste(w, collapse = "-") # needed later to store result
#     
#     for(strategy in sentixDataNamesReg){
#         SentData <- get(strategy)
#         rownames(SentData) <- as.integer(as.Date(rownames(SentData))) # for faster comparison below -> cast date to integer
#         erg <- matrix(NA, nrow = length(datesEvalLast)+1, ncol = numAsset) # +1 to lookup every weight
#         rownames(erg) <- c("1000-01-01", paste(datesEvalLast))
#         erg[1, ] <- rep(1/numAsset, numAsset)
#         
#         for(d in datesEvalLast){
#             dInd <- which(datesEvalLast==d)
#             
#             SSent <- cov(SentData[(which(rownames(SentData) == d)-sentLookback):
#                                       which(rownames(SentData) == d) - 1, ]) # -1 to just look in past
#             rdat <- ret[unique(pmax(which(rownames(ret)<=d) - 1,1)),] # from beginning to one day in past
#             muStock <- colMeans(rdat)
#             SStock <- cov(rdat)
#             
#             erg[dInd+1,] <- donlp2NLP(start = erg[dInd,], obj = hWeighted, 
#                          par.lower = rep(0, numAsset), ineqA = IneqA, 
#                          ineqA.lower = 1.0, ineqA.upper = 1.0)$solution
#         }
#         
#         E[[weightName]][[strategy]] <- erg
#         tt[(weightInd-1)*nrow(grid) + which(sentixDataNamesReg == strategy)] <- proc.time()[3]
#     }
# }
# save(E, file = file.path(folderData, "Optimization", paste0("Eserver_", format(Sys.time(), "%Y-%m-%d---%H-%M"))))
```


# END detach

```{r}
detach("package:FRAPO")
detach("package:fPortfolio")
detach("package:fAssets")
detach("package:fBasics")
detach("package:timeSeries")
```

